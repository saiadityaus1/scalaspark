package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

object lappractise {  
  
  case class schema(id:String,name:String,docname:String)

  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
    
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",                                                                                                                      
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "earliest")
      
    val topics = Array("tk10")
    val stream = KafkaUtils.createDirectStream[String, String](  
      ssc,
      PreferConsistent,                                                                                                 
         
      Subscribe[String, String](topics, kafkaParams))

    val kafkastream = stream.map(record => (record.key, record.value))
          val inputStream1 = kafkastream.map(rec => rec._2)
          
          inputStream1.print

     // val inputStream = kafkastream.map(rec => rec._2).map(line => line.split( ",")) 
    
   /* inputStream.foreachRDD(rdd=>
      {
        if(!rdd.isEmpty){
          
          val df1 = rdd.map(x=>(x(0).toInt,x(1),x(2),x(3),x(4).toInt)).toDF()
          df1.show()
          
        }
      }
      )*/
 
    
    ssc.start()
    ssc.awaitTermination()

  }

  
  
}




----------------------------------------

package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

object lappractise1 {  
  
  case class schema(id:String,location:String,date:String,total:Float)

  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
    
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "earliest")

    val topics = Array("tk4")
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams))

    val data=stream.map(x=>(x.key,x.value))
    
    val inputStream = data.map(rec => rec._2).print()
    
   // val inputStream = data.map(rec => rec._2).map(line => line.split("|"))
    //val data1= data.flatMap(x => x.mkString("|"))
    
    /*inputStream.foreachRDD(rdd=>
      {
        if(!rdd.isEmpty){
          rdd.map(x => x.mkString(","))
          val df = rdd.map(x => schema(x(0).toString(),x(1).toString(), x(2).toString, x(3).toFloat))
          
          val df1 = df.toDF()
          df1.show()
          
        }
      }
      )*/
      
    
    
    
   
    
    //(x=>schema(x(0),x(1),x(2),x(3).toFloat))
    
    
    
    
    
    ssc.start()
    ssc.awaitTermination()

  }

  
  
}

-------------------------------------------------

package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._
import org.apache.spark.SparkContext
import org.apache.spark.rdd.JdbcRDD
import java.sql.{Connection, DriverManager, ResultSet}
import java.util.Properties

object lappractise3 {  
  
  case class schema(id:Int,name:String,tablet:String,gender:String,amt:Int)

  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      .master("local")
      
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
    
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",                                                                                                                      
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "latest")
      
      val prop = new Properties() 
       prop.put("user", "root")
       prop.put("password", "root")
       
       
    val topics = Array("tk10")
    val stream = KafkaUtils.createDirectStream[String, String](  
      ssc,
      PreferConsistent,                                                                                                 
         
      Subscribe[String, String](topics, kafkaParams))

    val kafkastream = stream.map(record => (record.key, record.value))
          
      val inputStream1 = kafkastream.map(rec => rec._2).print()

      val inputStream = kafkastream.map(rec => rec._2)
      
    
    
     inputStream.foreachRDD(rdd=>
      {
        if(!rdd.isEmpty){
          val df1 = rdd.map(line => line.split(",")).map(x=>schema(x(0).toInt,x(1),x(2),x(3),x(4).toInt)).toDF()
          val df2=df1.filter($"gender"==="male")
          df2.show()
          df2.repartition(3).write.mode("overwrite").jdbc("jdbc:mysql://localhost/stream", "stream", prop)
          println("written to mysql")
         
        }
      }
      )  
      
      
    ssc.start()
    ssc.awaitTermination()

  }
}

-------------------------------------------


package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

object lappractise4 {  
  
  case class schema(id:Int,name:String,tablet:String,gender:String,amt:Int)

  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
    
    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "earliest")

    val topics = Array("tk6")
    val stream = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams))

   val kafkastream = stream.map(record => (record.key, record.value))
          
      val inputStream1 = kafkastream.map(rec => rec._2).print()

      val inputStream = kafkastream.map(rec => rec._2)
      
    
    
     
    inputStream.foreachRDD(rdd=>
      {
        if(!rdd.isEmpty){
          
         
          val df1 = rdd.map(line => line.split(",")).map(x=>schema(x(0).toInt,x(1),x(2),x(3),x(4).toInt)).toDF()
          
         
          df1.show()          
        }
      }
      )  
      
    
    
    
   
    
    //(x=>schema(x(0),x(1),x(2),x(3).toFloat))
    
    
    
    
    
    ssc.start()
    ssc.awaitTermination()

  }

  
  
}

---------------------------------------------

package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._
import java.util.Properties

object practise1 {
  
  case class schema(id:Int,name:String,tablet:String,gender:String,amt:Int)
   def main(args: Array[String])
   {
     
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
       val prop = new Properties() 
       prop.put("user", "root")
       prop.put("password", "root")
       
       
       
       val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",                                                                                                                      
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "earliest")
      
    val topics = Array("tk10","tk11")
    val stream = KafkaUtils.createDirectStream[String, String](  
      ssc,
      PreferConsistent,                                                                                                 
         
      Subscribe[String, String](topics, kafkaParams))
      
      val kafkastream= stream.map(x=>x.value)
      kafkastream.print
      
      kafkastream.foreachRDD(x=>
        {
        if(!x.isEmpty){
          
          val data=x.map(x=>x.split(",")).map(x=>schema(x(0).toInt,x(1),x(2),x(3),x(4).toInt)).toDF()
          
          data.show()
          
          data.repartition(3).write.mode("overwrite").jdbc("jdbc:mysql://localhost/stream", "stream", prop)
         
          println("data written to sql")
          
        }
      }
        
      
      )
      
      
      
       
       ssc.start()
    ssc.awaitTermination()
   }
}

----------------------------------------
package main.scala.streamingworkouts
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{ Seconds, StreamingContext }
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.storage.StorageLevel
import scala.collection.mutable.ListBuffer
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{ Put }
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.{ LongWritable, Writable, IntWritable, Text }
import org.apache.hadoop.mapred.{ TextOutputFormat, JobConf }
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.streaming.{Seconds, StreamingContext}
import StreamingContext._
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._
import org.elasticsearch.spark.streaming._
import org.apache.spark.sql.Row;
import org.elasticsearch.spark._
import java.util.Date
import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._
import java.util.Properties

object practise2 {
  
  case class schema(id:Int,name:String,tablet:String,gender:String,amt:Int)
   def main(args: Array[String])
   {
     
    val sparkConf = new SparkConf().setAppName("kafkahbase").setMaster("local[*]")
    val sparkcontext = new SparkContext(sparkConf)
    sparkcontext.setLogLevel("ERROR")
    val ssc = new StreamingContext(sparkcontext, Seconds(2))
    ssc.checkpoint("checkpointdir")
    val spark = SparkSession
      .builder()
      //.config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()
      
      import spark.implicits._
       import spark.sql
       val prop = new Properties() 
       prop.put("user", "root")
       prop.put("password", "root")
       
       
       
       val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "localhost:9092",                                                                                                                      
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> "kafka1",
      "auto.offset.reset" -> "earliest")
      
    val topics = Array("tk30")
    val stream = KafkaUtils.createDirectStream[String, String](  
      ssc,
      PreferConsistent,                                                                                                 
         
      Subscribe[String, String](topics, kafkaParams))
      
      val kafkastream= stream.map(x=>x.value)
      kafkastream.print
      
      
      /*kafkastream.foreachRDD(x=>
        {
        if(!x.isEmpty){
          
          val data=x.map(x=>x.split(",")).map(x=>schema(x(0).toInt,x(1),x(2),x(3),x(4).toInt)).toDF()
          
          data.show()
          
          data.repartition(3).write.mode("overwrite").jdbc("jdbc:mysql://localhost/stream", "stream", prop)
         
          println("data written to sql")
          
        }
      }
        
      
      )*/
      
      
      
       
       ssc.start()
    ssc.awaitTermination()
   }
}

-------------------------------------------
package main.scala.sparksqlworkouts

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.functions._


object dataframeapioperations
{
  def main(args:Array[String])
  {
       val conf = new SparkConf().setAppName("dataframeapioperations").setMaster("local")
       val sc = new SparkContext(conf)
       sc.setLogLevel("ERROR")
       val sqlContext = new SQLContext(sc)
       import sqlContext.implicits._ 
       val df = sqlContext.read.option("header","true")
       .option("delimiter", ",")
       .csv("file:///home/hduser/sparkdata/trans")
       
       df.printSchema()
       
       print(s"Number of Partitions: $df.rdd.getNumPartitions")
       
       
       df.groupBy("state").count().show()
       //caching
       df.cache()
       df.unpersist()
       
       df.filter(df("payment") === "cash" && col("amount") >= 0)
       .groupBy($"productname", $"city", $"state")
       .agg($"productname", $"city", $"state",count($"transid"),sum($"amount"))
       .sort(asc("productname"),asc("city"))
       .show(true)      
  }
       
}

--------------------------------
